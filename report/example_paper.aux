\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{huang2007credit}
\citation{poplin2018prediction}
\citation{tollenaar2013method}
\citation{kohavi1996scaling}
\citation{kohavi1996scaling}
\citation{zafar2015fairness}
\citation{agarwal2018reductions}
\newlabel{submission}{{1}{1}{}{section.1}{}}
\citation{bobko2004four}
\citation{hu2018short}
\citation{goodfellow2016deep}
\citation{zafar2015fairness}
\citation{zafar2015fairness}
\newlabel{q-rule}{{2}{2}{}{equation.2.2}{}}
\newlabel{nn-def}{{3}{2}{}{equation.3.3}{}}
\newlabel{bce}{{4}{2}{}{equation.3.4}{}}
\citation{paszke2017pytorch}
\citation{kingma2014adam}
\newlabel{vanilla-nn}{{1}{3}{For the vanilla neural network, normalized histograms of test set estimated probabilities that males (left) and females (right) make over 50K a year. Dotted black lines indicate the means of each distribution, which is 0.298 for males and 0.120 for females}{figure.1}{}}
\citation{langley00}
\citation{anonymous}
\newlabel{author info}{{6.3}{5}{}{subsection.6.3}{}}
\newlabel{final author}{{6.3.2}{5}{}{subsubsection.6.3.2}{}}
\newlabel{icml-historical}{{2}{6}{Historical locations and number of accepted papers for International Machine Learning Conferences (ICML 1993 -- ICML 2008) and International Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was produced, the number of accepted papers for ICML 2008 was unknown and instead estimated}{figure.2}{}}
\newlabel{alg:example}{{1}{6}{}{algorithm.1}{}}
\newlabel{sample-table}{{1}{6}{Classification accuracies for naive Bayes and flexible Bayes on various data sets}{table.1}{}}
\citation{Samuel59}
\citation{Samuel59}
\citation{kearns89,Samuel59,mitchell80}
\citation{MachineLearningI}
\citation{Samuel59}
\citation{langley00}
\citation{Newell81}
\citation{DudaHart2nd}
\citation{MachineLearningI}
\citation{mitchell80}
\citation{kearns89}
\citation{langley00}
\bibdata{example_paper}
\bibcite{agarwal2018reductions}{{1}{2018}{{Agarwal et~al.}}{{Agarwal, Beygelzimer, Dud{\'\i }k, Langford, and Wallach}}}
\bibcite{bobko2004four}{{2}{2004}{{Bobko \& Roth}}{{Bobko and Roth}}}
\bibcite{goodfellow2016deep}{{3}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, and Courville}}}
\bibcite{hu2018short}{{4}{2018}{{Hu \& Chen}}{{Hu and Chen}}}
\bibcite{huang2007credit}{{5}{2007}{{Huang et~al.}}{{Huang, Chen, and Wang}}}
\bibcite{kingma2014adam}{{6}{2014}{{Kingma \& Ba}}{{Kingma and Ba}}}
\bibcite{kohavi1996scaling}{{7}{1996}{{Kohavi}}{{}}}
\bibcite{paszke2017pytorch}{{8}{2017}{{Paszke et~al.}}{{Paszke, Gross, Chintala, and Chanan}}}
\bibcite{poplin2018prediction}{{9}{2018}{{Poplin et~al.}}{{Poplin, Varadarajan, Blumer, Liu, McConnell, Corrado, Peng, and Webster}}}
\bibcite{tollenaar2013method}{{10}{2013}{{Tollenaar \& Van~der Heijden}}{{Tollenaar and Van~der Heijden}}}
\bibcite{zafar2015fairness}{{11}{2015}{{Zafar et~al.}}{{Zafar, Valera, Rodriguez, and Gummadi}}}
\bibstyle{icml2019}
