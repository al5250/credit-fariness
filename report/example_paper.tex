%%%%%%%% ICML 2019 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath}
\usepackage{amsfonts}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathbb{V}\text{ar}}
\newcommand{\Cov}{\mathbb{C}\text{ov}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\Norm}{\mathcal{N}}
\newcommand\given[1][]{\:#1\vert\:}
\newcommand{\bd}[1]{\boldsymbol{#1}}
\newcommand{\hy}{\hat{y}}
\newcommand{\hp}{\hat{p}}
\newcommand{\zbar}{\bar{z}}
\newcommand{\hz}{\hat{z}}
\newcommand{\dbar}{\bar{d}}
\newcommand{\idx}[3][]{{#2}^{(#3)}_{#1}}
\newcommand{\bidx}[3][]{\bd{#2}^{(#3)}_{#1}}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2019} with \usepackage[nohyperref]{icml2019} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2019}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2019}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Optimization-Based Approaches for Enforcing Fairness in Machine Learning}

\begin{document}

\twocolumn[
\icmltitle{Optimization-Based Approaches for Enforcing Fairness in Machine Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2019
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Amil Merchant}{equal,harvard}
\icmlauthor{Alexander Lin}{equal,harvard}
\end{icmlauthorlist}

\icmlaffiliation{harvard}{Applied Mathematics 221, Harvard University, Cambridge, Massachusetts, USA}

\icmlcorrespondingauthor{Amil Merchant}{amilmerchant@college.harvard.edu}
\icmlcorrespondingauthor{Alexander Lin}{alexanderlin01@college.harvard.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
\end{abstract}

\section{Introduction}
\label{submission}

Over the past few years, machine learning (ML) and artificial intelligence (AI) have become increasingly more common for high-stakes decision making.  Researchers have proposed machine learning algorithms for applications such as credit scoring \citep{huang2007credit}, personalized medicine \citep{poplin2018prediction}, and redicivism prediction \citep{tollenaar2013method}.  

In light of our increased adoption of ML/AI methods, it is important that we do not allow these technologies to foster unfairness within our society.  Machine learning algorithms fundamentally rely on past data in order to function.   They attempt to generalize patterns found in the data and apply these patterns to make predictions in future scenarios.  However, in certain situations, historical injustices against presently protected subgroups of a population may have led to the recording of biased data.  Naively training a model on this biased data may lead to a biased algorithm that discriminates against these protected subgroups.  Subsequently using this algorithm for high-stakes decision making may lead to further injustices and bias the collection of future data, thereby leading to a dangerous positive feedback loop.  

Thus, finding ways to enforce fair predictions for machine learning algorithms is a problem of utmost importance.  In this paper, we propose some methods that strive to achieve this goal.  These methods are primarily optimization-based, meaning that they each involve augmenting the objective function of machine learning methods in some manner and can be seen as a form of regularization.  We employ our methods in neural networks, models that have garnered a great deal of popularity in recent years due to empirical success across many domains.  Our empirical results are presented on the \emph{adult income dataset}\footnote{This dataset is publicly available at \url{https://archive.ics.uci.edu/ml/datasets/adult}.}, which was collected from 1994 census data \citep{kohavi1996scaling}.  We show that our proposed approaches can significantly reduce model bias defined in the form of \emph{disparate impact} and uphold desired levels of \emph{demographic parity} without sacrificing a prohibitive amount of accuracy.             

\subsection{Related Work}
Talk about COMPAS, other work in fairness, etc.

\section{Background}

\subsection{Adult Income Dataset}

The adult income dataset \citep{kohavi1996scaling} contains data from $N = 32,561$ respondents to the 1994 United States Census.  Each person $n$ is characterized by $J = 14$ attributes, denoted $\bidx x n = \{\idx[1] x n, \ldots, \idx[J] x n\}$, including education level, occupation type, capital gains, capital losses, and number of hours worked per week.  The goal is to predict a binary variable $\idx y n \in \{0, 1\}$, which indicates whether or not person $n$ makes over \$50,000 a year.

In this case, the protected attributes $\bidx z n$ for person $n$ are their \emph{sex} and their \emph{race}.  Historical inequities have led to groups such as women and African Americans having significantly lower fractions of individuals making over \$50,000 a year.  Using a model naively trained on the adult income dataset for high stakes decision making in the present day -- such as estimating a person's income for loan approval or determining how much to pay a new hire -- may lead to heavily biased results.  Thus, there is motivation to incorporate predictive fairness into the model training process.

\subsection{Disparate Impact and Demographic Parity}   

\emph{Disparate impact} is the notion in which a model's biased classification process leads to outcomes that disproportionately hurt (or benefit) people with sensitive attributes.  It was first introduced by Zafar et al.  \yrcite{zafar2015fairness}.  Simply removing the sensitive attributes $\bd z$ from the dataset and training a model on the remaining attributes $\bd x \setminus \bd z$ may still yield biased predictions, because $\bd z$ may be correlated with the remaining subset \citep{agarwal2018reductions}.

To counter disparate impact, we wish to enforce \emph{demographic parity}, which demands that the distribution of scores for any protected classes is the same.  Let $\hat{p}(y = 1)$ be a model's prediction of the probability of class 1 in binary classification.  Formally, demographic parity is defined as:
\begin{align}
\hp(y = 1 \given \bd z = k_1) = \hp(y = 1 \given \bd z = k_2),
\end{align}  
where $k_1$ and $k_2$ are different realizations of the random variable $\bd z$.  For example, if $\bd z$ is sex, $k_1$ could be \texttt{Male} and $k_2$ could be \texttt{Female}.  Intuitively, this means that only changing the protected attribute $\bd z$ should not influence the predictions in any way.  

Using demographic parity as a definition of machine learning fairness offers some advantages.  First and foremost, there exists legal support for this definition in the United States.  In 1978, four government agencies -- including the EEOC, Department of Labor, Department of Justice, and the Civil Service Commission -- proposed the four-fifths (or 80\%) rule as a benchmark with assessing adverse disparate impact for protected classes \citep{bobko2004four}.  Specifically, these agencies required that
\begin{align}
\min \left\{\frac{\hp(y = 1 \given \bd z = k_1)}{\hp(y = 1 \given \bd z = k_2)}, \frac{\hp(y = 1 \given \bd z = k_2)}{\hp(y = 1 \given \bd z = k_1)} \right\} \geq \frac{q}{100} \label{q-rule}
\end{align} 
where $q = 80$ in the legal definition.  Note that $q = 100$ corresponds to zero disparate impact and complete demographic parity.  Recently, Hu and Chen \yrcite{hu2018short} additionally argue that short-term enforcement of demographic parity has long-term benefits for countering discrimination against minorities in the labor market.  

\section{Methods for Enforcing Demographic Parity}
We present two optimization-based methods for enforcing demographic parity in neural networks.  

A neural network is a cascade of linear and nonlinear transformations of the input vector $\bd x$ to yield an output vector $\bd h_L$ \citep{goodfellow2016deep}.  An $L$-layer neural network can be described by the equations,
\begin{align}
\bd h_1 &= \idx f {1}(\idx W {1} \bd x + \idx b {1}), & \quad \ldots  \label{nn-def} \\
\bd h_\ell &= \idx f {\ell}(\idx W {\ell} \bd h_{\ell - 1} + \idx b {\ell}), & \quad \ldots\nonumber \\
\bd h_L &= \idx f {L}(\idx W {L} \bd h_{L-1} + \idx b {L}), \nonumber
\end{align}  
where each pair $(\idx W \ell, \idx b \ell)$ parameterizes an affine transformation (via matrix multiplication and bias addition), each $\idx f \ell$ is a nonlinear function applied element-wise, and each $\bd h_\ell$ denotes an intermediary hidden state representation of the input.  

In binary classifiers, it is common to let $\idx W L$ be a row vector, $\idx b L$ be a single scalar, and $\idx f L$  be the sigmoid function $\sigma(a) = 1 / (1 + \exp (- a))$.  Such constraints force the final output $\hat{p} = \bd h_L$ to be a scalar within the range $[0, 1]$, which allows us to interpret it as the estimated probability of $y = 1$.  For selected nonlinearities $\{\idx f \ell\}_{\ell=1}^L$, the weights $\{\idx W \ell\}_{\ell=1}^L$ and biases $\{\idx b \ell\}_{\ell=1}^L$ are trained to minimize the \emph{binary cross-entropy loss} $Q_0$ over the entire dataset, which is defined as
\begin{align}
Q_0 = \sum_{n=1}^N \idx y n \log \idx \hp n + (1 - \idx y n) \log (1 - \idx \hp n), \label{bce}
\end{align}         
where each $\idx \hp n$ is generated by passing $\bidx x n$ through the neural network.

\subsection{Regularizing Decision Boundary Covariance} \label{method1}
Zafar et al. \yrcite{zafar2015fairness} propose regularizing the covariance between the distance to the decision boundary of a classifier and the protected classes $\bd z$ to enforce demographic parity.  They apply their framework to logistic regression and support vector machines.  We generalize this method to working with neural networks.

Using the neural network binary classifier of Equation \ref{nn-def}, we define the \emph{decision boundary distance} $\idx d n$ of training example $n$ as the value obtained before the final nonlinearity, i.e. 
\begin{align}
\idx d n = \idx W {L} \bidx[L-1] h n + \idx b {L}.
\end{align} 
To see why $\idx d n$ is related to the decision boundary of the neural network classifier, observe that the estimated probability of $\idx y n = 1$ is $\idx \hp n = \sigma(\idx d n)$.  Thus, if $\idx d n > 0$, then $\idx \hp n > 1/2$ (so it makes more sense to classify $n$ as class 1) and if $\idx d n < 0$, then $\idx \hp n < 1/2$ (so it makes more sense to classify $n$ as class 0).  Thus, the variable $d$ encodes a scale centered at zero and characterizes the confidence of the classifier to classify as class 0 or class 1.  

If the covariance between the decision boundary distance $d$ and the protected attribute $\bd z$ is zero, then knowing $\bd z$ should have no impact on knowing $p(y \given \bd x)$, which is the definition of satisfying demographic parity.  We can empirically estimate this covariance by observing the following:
\begin{align}
\Cov(\bd z, d) &= \E[(\bd z - \bd \zbar) \cdot (d - \dbar)] \\
&= \E[(\bd z - \bd \zbar) \cdot d] - \E[(\bd z - \bd \zbar)] \cdot \dbar \nonumber \\
&= \E[(\bd z - \bd \zbar) \cdot d] - 0 \nonumber \\
&\approx \frac{1}{N} \sum_{n=1}^N (\bidx z n - \bd \hz) \cdot \idx d n, \nonumber
\end{align}      
where $\bd \hz = 1 / N \cdot \sum_{n=1}^N \bidx z n$.  Since Zafar et al. \yrcite{zafar2015fairness} work with only convex classifiers, they simply add the following convex constraint to their logistic regression and support vector machine settings:
\begin{align}
\left | \frac{1}{N} \sum_{n=1}^N (\bidx z n - \bd \hz) \cdot \idx d n \right | \leq \bd c,
\end{align}
for some constant $\bd c$ corresponding to the level of desired demographic parity.  In our neural network setting, we instead directly add the empirical covariance as a penalized regularization term to the binary cross entropy objective function of Equation \ref{bce}.  Thus, the full objective function is
\begin{align}
Q_1 &= \sum_{n=1}^N \idx y n \log \idx \hp n + (1 - \idx y n) \log (1 - \idx \hp n) \label{penalty-bce} \\ 
&\quad + \lambda \cdot \left | \frac{1}{N}  \sum_{n=1}^N (\bidx z n - \bd \hz) \cdot \idx d n \right|, \nonumber
\end{align}
where $\lambda$ controls the degree of regularization.  Increasing $\lambda$ will increase the penalty of the covariance and ideally lead to greater demographic parity.  We wish to adjust $\lambda$ so that it is large enough to satisfy fairness constraints, yet small enough to not prohibitively affect classifier accuracy.    

\subsection{Regularizing Representation Space Bias}

\section{Results}
Our empirical results are evaluated on the adult income dataset.  We first naively train a vanilla neural network and show how it suffers from disparate impact.  Then, we apply our methods for enforcing demographic parity to exhibit how this disparate impact can be mitigated.  All experiments are implemented using the PyTorch deep learning library \citep{paszke2017pytorch}.  

\subsection{Vanilla Neural Network} \label{vanilla-net}

We train a simple neural network with $L = 2$ layers that performs well on the adult income dataset.  The input is $\bd x \setminus \bd z$, the set of all attributes minus sex and race.  The single hidden layer $\bidx h 1$ has 64 hidden units.  We let $\idx f 1$ be the rectified linear (ReLU) function and $\idx f 2$ be the sigmoid function.  Weights and biases $\{\idx W 1, \idx W 2, \idx b 1, \idx b 2\}$ are initialized as $\mathcal{N}(0, 1)$ random variables.  

We divide the dataset of $N = 32,561$ individuals into a training set $\mathcal{D}_\text{train}$ of $26,048$ people and test set $\mathcal{D}_\text{test}$ of $6,513$ people, which roughly corresponds to an 80\%-20\% split.  The network is trained using the binary cross entropy loss function of Equation \ref{bce} on $\mathcal{D}_\text{train}$.  For optimization, we use the ADAM stochastic optimizer \citep{kingma2014adam} with a minibatch of $1,024$ examples.  The network is trained for 20 epochs, which are defined as passes through the entire training set.   

Evaluation is performed on the test set.  Test set accuracy is 85.00\%, which is decent.  However, there are gross violations of demographic parity.  

If we observe the distributions over estimated probabilities of making over 50K divided by sex (i.e. \texttt{Male} vs. \texttt{Female}), we see that there are significant discrepancies.  Figure \ref{vanilla-nn} traces the distribution of $\idx \hp n \given \bidx z n = \texttt{Male}$ and $\idx \hp n \given \bidx z n = \texttt{Female}$ for all $n \in \mathcal{D}_\text{test}$ by using simple kernel density estimation.  The shapes are quite different.  Let $\mathcal{D}_\text{test}^\texttt{Male}$ and $\mathcal{D}_\text{test}^\texttt{Female}$ be partitions of $\mathcal{D}_\text{test}$ based on sex.  We see that the largest possible $q$ that satisfies Equation \ref{q-rule} is $q = 41.82\%$, where $q$ is found empirically in this example as 
\begin{align}
q = \frac{| \mathcal{D}_\text{test}^\texttt{Female} |^{-1} \sum_{n \in \mathcal{D}_\text{test}^\texttt{Female}} \idx \hp n}{| \mathcal{D}_\text{test}^\texttt{Male} |^{-1} \sum_{n \in \mathcal{D}_\text{test}^\texttt{Male}} \idx \hp n}.
\end{align} 

This model exhibits significant bias against females, likely because it was trained on a biased dataset.  Thus, it is unsuitable for use in future high-stakes decision making, such as determining how much a female should make or estimating a female's income for loan approval. 

We can repeat the same exercise for race on analogously defined datasets $\mathcal{D}_\text{test}^\texttt{Minorities}$ and $\mathcal{D}_\text{test}^\texttt{White}$.  For race, we find that $q = 63.63\%$, which is less unfair, yet still violates the 80\% rule used in legal settings.  Figure \ref{vanilla-nn} presents the corresponding plot.

Mean predicted probabilities of high-income for the aforementioned sensitive groups can be found in Table \ref{vanilla-table}.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{img/vanilla-bias.pdf}}
\caption{For the vanilla neural network, fitted kernel density estimations of test set estimated probabilities that different races (left) and different sexes (right) make over 50K a year.  Dotted lines indicate the means of each distribution (Table \ref{vanilla-table}).}
\label{vanilla-nn}
\end{center}
\vskip -0.2in
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{c|c||c |c} 
 \hline
 Female & Male & Minorities & White \\ [0.5ex] 
 0.125 & 0.300 & 0.162 & 0.256 \\
 \hline
\end{tabular}
\caption{For the vanilla neural network, test set mean estimated probabilities of making over 50K for various sensitive groups.} \label{vanilla-table}
\end{table}

\subsection{Regularizing Decision Boundary Covariance}

We apply the method described in Section \ref{method1} to correcting disparate impact for the vanilla neural network of Section \ref{vanilla-net}.  In doing so, we keep the same general architecture and training hyperparameters described in the previous section.  However, instead of training the network using normal binary cross-entropy loss $Q_0$  (Equation \ref{bce}), we instead use the regularized objective $Q_1$ that penalizes decision boundary covariance (Equation \ref{penalty-bce}).  

In our experiments, we vary the regularization penalty $\lambda$ to show corresponding effects on the final accuracy and fairness of the neural network classifier.  We try values of $\lambda$ within the set $\{3 \times 10^{-2}, 1 \times 10^{-2}, 3 \times 10^{-3}, 1 \times 10^{-3}, 3 \times 10^{-4}, 1 \times 10^{-4}\}$, which covers approximate increases in factors of three.  

Graphs and a table of the results for sex on the training and test sets can be found in Figure \ref{boundary-fig-sex} and Table \ref{boundary-table-sex}, respectively.  We see that choosing a suitable $\lambda$ can satisfy demographic parity without sacrificing significant amounts of accuracy.  Looking at Figure \ref{boundary-fig-sex} and Table \ref{boundary-table-sex}, there is a sharp bend in the curve for $\lambda = 3 \times 10^{-3}$, so this is an appropriate final choice.

Similar results for race can be found in Figure \ref{boundary-fig-race} and Table \ref{boundary-table-race}.  Looking at these values, it appears that $\lambda = 1 \times 10^{-3}$ is a reasonable choice here.  Figure \ref{boundary-kde}   


\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{img/boundary-tradeoff-sex.pdf}}
\caption{For the vanilla neural network, fitted kernel density estimations of test set estimated probabilities that different races (left) and different sexes (right) make over 50K a year.  Dotted lines indicate the means of each distribution (Table \ref{vanilla-table}).}
\label{boundary-fig-sex}
\end{center}
\vskip -0.2in
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{c|cc |cc} 
 \hline
 \hline
 $\lambda$ (for Sex) & Train Acc & Train $q$ & Test Acc & Test $q$ \\ 
 \hline
 $3 \times 10^{-2}$ & 0.759 & \textbf{0.994} & 0.759 & \textbf{0.996}\\
 $1 \times 10^{-2}$ &0.777& 0.967 & 0.778 & 0.977\\
$3 \times 10^{-3}$ & 0.834 & 0.934 & 0.838 & 0.957 \\
$1 \times 10^{-3}$ & 0.840 & 0.895 & 0.838 &0.922 \\
$3 \times 10^{-4}$ & 0.849 & 0.771 & 0.843 & 0.801\\
$1 \times 10^{-4}$ & \textbf{0.852} & 0.529 & \textbf{0.850} & 0.547\\
 \hline
\end{tabular}
\caption{For the vanilla neural network, test set mean estimated probabilities of making over 50K for various sensitive groups.} \label{boundary-table-sex}
\end{table}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{img/boundary-tradeoff-race.pdf}}
\caption{For the vanilla neural network, fitted kernel density estimations of test set estimated probabilities that different races (left) and different sexes (right) make over 50K a year.  Dotted lines indicate the means of each distribution (Table \ref{vanilla-table}).}
\label{boundary-fig-race}
\end{center}
\vskip -0.2in
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{c|cc |cc} 
 \hline
 \hline
 $\lambda$ (for Race) & Train Acc & Train $q$ & Test Acc & Test $q$ \\ 
 \hline
 $3 \times 10^{-2}$ & 0.759 & \textbf{0.994} & 0.759 & \textbf{0.996}\\
 $1 \times 10^{-2}$ &0.819 & 0.967 & 0.822 & 0.960\\
$3 \times 10^{-3}$ & 0.846 & 0.937 & 0.841 & 0.913\\
$1 \times 10^{-3}$ &0.848 &0.930 &  0.845 &0.910 \\
$3 \times 10^{-4}$ & 0.852 & 0.822 & 0.846 & 0.810\\
$1 \times 10^{-4}$ & \textbf{0.853} & 0.707 & \textbf{0.849} & 0.700\\
 \hline
\end{tabular}
\caption{For the vanilla neural network, test set mean estimated probabilities of making over 50K for various sensitive groups.} \label{boundary-table-race}
\end{table}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{img/boundary-bias.pdf}}
\caption{Blah blah blah}
\label{vanilla-nn}
\end{center}
\vskip -0.2in
\end{figure}


  

\subsection{Regularizing Representation Space Bias}

\section{Discussion and Conclusion}

\textbf{Paper Deadline:} The deadline for paper submission that is
advertised on the conference website is strict. If your full,
anonymized, submission does not reach us on time, it will not be
considered for publication. 

\textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
author information may appear on the title page or in the paper
itself. Section~\ref{author info} gives further details.

\textbf{Simultaneous Submission:} ICML will not accept any paper which,
at the time of submission, is under review for another conference or
has already been published. This policy also applies to papers that
overlap substantially in technical content with conference papers
under review or previously published. ICML submissions must not be
submitted to other conferences during ICML's review period. Authors
may submit to ICML substantially different versions of journal papers
that are currently under review by the journal, but not yet accepted
at the time of submission. Informal publications, such as technical
reports or papers in workshop proceedings which do not appear in
print, do not fall under these restrictions.

\medskip

Authors must provide their manuscripts in \textbf{PDF} format.
Furthermore, please make sure that files contain only embedded Type-1 fonts
(e.g.,~using the program \texttt{pdffonts} in linux or using
File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
might come from graphics files imported into the document.

Authors using \textbf{Word} must convert their document to PDF\@. Most
of the latest versions of Word have the facility to do this
automatically. Submissions will not be accepted in Word format or any
format other than PDF\@. Really. We're not joking. Don't send Word.

Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
Those using \texttt{latex} and \texttt{dvips} may need the following
two commands:

{\footnotesize
\begin{verbatim}
dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
ps2pdf paper.ps
\end{verbatim}}
It is a zero following the ``-G'', which tells dvips to use
the config.pdf file. Newer \TeX\ distributions don't always need this
option.

Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
results. This program avoids the Type-3 font problem, and supports more
advanced features in the \texttt{microtype} package.

\textbf{Graphics files} should be a reasonable size, and included from
an appropriate format. Use vector formats (.eps/.pdf) for plots,
lossless bitmap formats (.png) for raster graphics with sharp lines, and
jpeg for photo-like images.

The style file uses the \texttt{hyperref} package to make clickable
links in documents. If this causes problems for you, add
\texttt{nohyperref} as one of the options to the \texttt{icml2019}
usepackage statement.


\subsection{Submitting Final Camera-Ready Copy}

The final versions of papers accepted for publication should follow the
same format and naming convention as initial submissions, except that
author information (names and affiliations) should be given. See
Section~\ref{final author} for formatting instructions.

The footnote, ``Preliminary work. Under review by the International
Conference on Machine Learning (ICML). Do not distribute.'' must be
modified to ``\textit{Proceedings of the
$\mathit{36}^{th}$ International Conference on Machine Learning},
Long Beach, USA, 2019.
Copyright 2019 by the author(s).''

For those using the \textbf{\LaTeX} style file, this change (and others) is
handled automatically by simply changing
$\mathtt{\backslash usepackage\{icml2019\}}$ to
$$\mathtt{\backslash usepackage[accepted]\{icml2019\}}$$
Authors using \textbf{Word} must edit the
footnote on the first page of the document themselves.

Camera-ready copies should have the title of the paper as running head
on each page except the first one. The running title consists of a
single line centered above a horizontal rule which is $1$~point thick.
The running head should be centered, bold and in $9$~point type. The
rule should be $10$~points above the main text. For those using the
\textbf{\LaTeX} style file, the original title is automatically set as running
head using the \texttt{fancyhdr} package which is included in the ICML
2019 style file package. In case that the original title exceeds the
size restrictions, a shorter form can be supplied by using

\verb|\icmltitlerunning{...}|

just before $\mathtt{\backslash begin\{document\}}$.
Authors using \textbf{Word} must edit the header of the document themselves.

\section{Format of the Paper}

All submissions must follow the specified format.

\subsection{Length and Dimensions}

Submitted papers can be up to eight pages long, not including references, and up to twelve pages when references and acknowledgments are included.
Acknowledgements should be limited to grants and people who contributed to the paper.
Any submission that exceeds
this page limit, or that diverges significantly from the specified format,
will be rejected without review.

The text of the paper should be formatted in two columns, with an
overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
between the columns. The left margin should be 0.75~inches and the top
margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
whether you print on US letter or A4 paper, but all final versions
must be produced for US letter size.

The paper body should be set in 10~point type with a vertical spacing
of 11~points. Please use Times typeface throughout the text.

\subsection{Title}

The paper title should be set in 14~point bold type and centered
between two horizontal rules that are 1~point thick, with 1.0~inch
between the top rule and the top edge of the page. Capitalize the
first letter of content words and put the rest of the title in lower
case.

\subsection{Author Information for Submission}
\label{author info}

ICML uses double-blind review, so author information must not appear. If
you are using \LaTeX\/ and the \texttt{icml2019.sty} file, use
\verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
will not be printed unless \texttt{accepted} is passed as an argument to the
style file.
Submissions that include the author information will not
be reviewed.

\subsubsection{Self-Citations}

If you are citing published papers for which you are an author, refer
to yourself in the third person. In particular, do not use phrases
that reveal your identity (e.g., ``in previous work \cite{langley00}, we
have shown \ldots'').

Do not anonymize citations in the reference section. The only exception are manuscripts that are
not yet published (e.g., under submission). If you choose to refer to
such unpublished manuscripts \cite{anonymous}, anonymized copies have
to be submitted
as Supplementary Material via CMT\@. However, keep in mind that an ICML
paper should be self contained and should contain sufficient detail
for the reviewers to evaluate the work. In particular, reviewers are
not required to look at the Supplementary Material when writing their
review.

\subsubsection{Camera-Ready Author Information}
\label{final author}

If a paper is accepted, a final camera-ready copy must be prepared.
%
For camera-ready papers, author information should start 0.3~inches below the
bottom rule surrounding the title. The authors' names should appear in 10~point
bold type, in a row, separated by white space, and centered. Author names should
not be broken across lines. Unbolded superscripted numbers, starting 1, should
be used to refer to affiliations.

Affiliations should be numbered in the order of appearance. A single footnote
block of text should be used to list all the affiliations. (Academic
affiliations should list Department, University, City, State/Region, Country.
Similarly for industrial affiliations.)

Each distinct affiliations should be listed once. If an author has multiple
affiliations, multiple superscripts should be placed after the name, separated
by thin spaces. If the authors would like to highlight equal contribution by
multiple first authors, those authors should have an asterisk placed after their
name in superscript, and the term ``\textsuperscript{*}Equal contribution"
should be placed in the footnote block ahead of the list of affiliations. A
list of corresponding authors and their emails (in the format Full Name
\textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
Ideally only one or two names should be listed.

A sample file with author names is included in the ICML2019 style file
package. Turn on the \texttt{[accepted]} option to the stylefile to
see the names rendered. All of the guidelines above are implemented
by the \LaTeX\ style file.

\subsection{Abstract}

The paper abstract should begin in the left column, 0.4~inches below the final
address. The heading `Abstract' should be centered, bold, and in 11~point type.
The abstract body should use 10~point type, with a vertical spacing of
11~points, and should be indented 0.25~inches more than normal on left-hand and
right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
sentences. Gross violations will require correction at the camera-ready phase.

\subsection{Partitioning the Text}

You should organize your paper into sections and paragraphs to help
readers place a structure on the material and understand its
contributions.

\subsubsection{Sections and Subsections}

Section headings should be numbered, flush left, and set in 11~pt bold
type with the content words capitalized. Leave 0.25~inches of space
before the heading and 0.15~inches after the heading.

Similarly, subsection headings should be numbered, flush left, and set
in 10~pt bold type with the content words capitalized. Leave
0.2~inches of space before the heading and 0.13~inches afterward.

Finally, subsubsection headings should be numbered, flush left, and
set in 10~pt small caps with the content words capitalized. Leave
0.18~inches of space before the heading and 0.1~inches after the
heading.

Please use no more than three levels of headings.

\subsubsection{Paragraphs and Footnotes}

Within each section or subsection, you should further partition the
paper into paragraphs. Do not indent the first line of a given
paragraph, but insert a blank line between succeeding ones.

You can use footnotes\footnote{Footnotes
should be complete sentences.} to provide readers with additional
information about a topic without interrupting the flow of the paper.
Indicate footnotes with a number in the text where the point is most
relevant. Place the footnote in 9~point type at the bottom of the
column in which it appears. Precede the first footnote in a column
with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
appear in each column, in the same order as they appear in the text,
but spread them across columns and pages if possible.}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
\caption{Historical locations and number of accepted papers for International
Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
produced, the number of accepted papers for ICML 2008 was unknown and instead
estimated.}
\label{icml-historical}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{Figures}

You may want to include figures in the paper to illustrate
your approach and results. Such artwork should be centered,
legible, and separated from the text. Lines should be dark and at
least 0.5~points thick for purposes of reproduction, and text should
not appear on a gray background.

Label all distinct components of each figure. If the figure takes the
form of a graph, then give a name for each axis and include a legend
that briefly describes each curve. Do not include a title inside the
figure; instead, the caption should serve this function.

Number figures sequentially, placing the figure number and caption
\emph{after} the graphics, with at least 0.1~inches of space before
the caption and 0.1~inches after it, as in
Figure~\ref{icml-historical}. The figure caption should be set in
9~point type and centered unless it runs two or more lines, in which
case it should be flush left. You may float figures to the top or
bottom of a column, and you may set wide figures across both columns
(use the environment \texttt{figure*} in \LaTeX). Always place
two-column figures at the top or bottom of the page.

\subsection{Algorithms}

If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
environments to format pseudocode. These require
the corresponding stylefiles, algorithm.sty and
algorithmic.sty, which are supplied with this package.
Algorithm~\ref{alg:example} shows an example.

\begin{algorithm}[tb]
   \caption{Bubble Sort}
   \label{alg:example}
\begin{algorithmic}
   \STATE {\bfseries Input:} data $x_i$, size $m$
   \REPEAT
   \STATE Initialize $noChange = true$.
   \FOR{$i=1$ {\bfseries to} $m-1$}
   \IF{$x_i > x_{i+1}$}
   \STATE Swap $x_i$ and $x_{i+1}$
   \STATE $noChange = false$
   \ENDIF
   \ENDFOR
   \UNTIL{$noChange$ is $true$}
\end{algorithmic}
\end{algorithm}

\subsection{Tables}

You may also want to include tables that summarize material. Like
figures, these should be centered, legible, and numbered consecutively.
However, place the title \emph{above} the table with at least
0.1~inches of space before the title and the same after it, as in
Table~\ref{sample-table}. The table title should be set in 9~point
type and centered unless it runs two or more lines, in which case it
should be flush left.

% Note use of \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines.

\begin{table}[t]
\caption{Classification accuracies for naive Bayes and flexible
Bayes on various data sets.}
\label{sample-table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
Data set & Naive & Flexible & Better? \\
\midrule
Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Tables contain textual material, whereas figures contain graphical material.
Specify the contents of each row and column in the table's topmost
row. Again, you may float tables to a column's top or bottom, and set
wide tables across both columns. Place two-column tables at the
top or bottom of the page.

\subsection{Citations and References}

Please use APA reference format regardless of your formatter
or word processor. If you rely on the \LaTeX\/ bibliographic
facility, use \texttt{natbib.sty} and \texttt{icml2019.bst}
included in the style-file package to obtain this format.

Citations within the text should include the authors' last names and
year. If the authors' names are included in the sentence, place only
the year in parentheses, for example when referencing Arthur Samuel's
pioneering work \yrcite{Samuel59}. Otherwise place the entire
reference in parentheses with the authors and year separated by a
comma \cite{Samuel59}. List multiple references separated by
semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
construct only for citations with three or more authors or after
listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

Authors should cite their own work in the third person
in the initial version of their paper submitted for blind review.
Please refer to Section~\ref{author info} for detailed instructions on how to
cite your own papers.

Use an unnumbered first-level section heading for the references, and use a
hanging indent style, with the first line of the reference flush against the
left margin and subsequent lines indented by 10 points. The references at the
end of this document give examples for journal articles \cite{Samuel59},
conference publications \cite{langley00}, book chapters \cite{Newell81}, books
\cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
\cite{mitchell80}, and dissertations \cite{kearns89}.

Alphabetize references by the surnames of the first authors, with
single author entries preceding multiple author entries. Order
references for the same authors by year of publication, with the
earliest first. Make sure that each reference includes all relevant
information (e.g., page numbers).

Please put some effort into making references complete, presentable, and
consistent. If using bibtex, please protect capital letters of names and
abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
in your .bib file.

\subsection{Software and Data}

We strongly encourage the publication of software and data with the
camera-ready version of the paper whenever appropriate. This can be
done by including a URL in the camera-ready copy. However, do not
include URLs that reveal your institution or identity in your
submission for review. Instead, provide an anonymous URL or upload
the material as ``Supplementary Material'' into the CMT reviewing
system. Note that reviewers are not required to look at this material
when writing their review.

% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}

\textbf{Do not} include acknowledgements in the initial version of
the paper submitted for blind review.

If a paper is accepted, the final camera-ready version can (and
probably should) include acknowledgements. In this case, please
place such acknowledgements in an unnumbered section at the
end of the paper. Typically, this will include thanks to reviewers
who gave useful comments, to colleagues who contributed to the ideas,
and to funding agencies and corporate sponsors that provided financial
support.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2019}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\section{Do \emph{not} have an appendix here}

\textbf{\emph{Do not put content after the references.}}
%
Put anything that you might normally include after the references in a separate
supplementary file.

We recommend that you build supplementary material in a separate document.
If you must create one PDF and cut it up, please be careful to use a tool that
doesn't alter the margins, and that doesn't aggressively rewrite the PDF file.
pdftk usually works fine. 

\textbf{Please do not use Apple's preview to cut off supplementary material.} In
previous years it has altered margins, and created headaches at the camera-ready
stage. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
